# LLM-from-Scratch-Pretrain-and-Pray
This repository is aimed to build a Transformer language model from scratch by training a classical OG encoder-decoder architechture, going through contextual and postional embeddings, attention layers and using Reddit Data to perform Pretraining (And praying everything is alright) on commodity hardware.
