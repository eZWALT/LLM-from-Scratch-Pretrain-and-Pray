{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Data Extraction\n",
    "\n",
    "In this initial section we are going to list all the data sources that are going to be used for this project and also all the related local data ingestions and uploads to HuggingFace, which we will use as our main repository for datasets and models. Right now we are going to list some of the available datasets:\n",
    "\n",
    "- [Common Corpus](https://huggingface.co/datasets/PleIAs/common_corpus): Real-world large scale dataset (Over 2 Trillion tokens)\n",
    "- [UpVoteWeb](https://huggingface.co/datasets/OpenCo7/UpVoteWeb): Large-scale reddit comments and posts containing final scores\n",
    "- [BurialGoods Transcripts](): My own hand-made dataset containing transcriptions of BurialGoods cinematographic pieces of art for pretraining\n",
    "- [Cursed Toxic Pretraining Collection](https://huggingface.co/collections/eZWALT/cursed-toxic-pretraining): Curated collection of HuggingFace\n",
    "\n",
    "If you are feeling brave (or dumb) enough, use the last corpus at your own risk! Now lets start cooking!\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"../resources/walterwhite.gif\" width=\"300\">\n",
    "</p>\n",
    "\n",
    "\n",
    "For each dataset we will have to define an personalized simple **local preprocessing**, which for pretraining data that consists of raw strings can be as simple as removing unnecessary fields and columns (categorical and numerical variables) and concatenating multiple string fields or subsetting. Optionally, we can also at the end define some **global preprocessing** unified steps such as special character removal or other simple steps that do not require great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas \n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Preprocess BurialGoods Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset, Features, Value\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "\n",
    "def iter_samples_from_zips(data_dir, selected_zips, k_per_zip):\n",
    "    \"\"\"\n",
    "    Generator yielding simplified sample dicts from selected zip files.\n",
    "    Fields are simplified to: text, title.\n",
    "    \"\"\"\n",
    "    for zip_path in tqdm(selected_zips, desc=\"Processing Zips\"):\n",
    "        with ZipFile(zip_path, \"r\") as z:\n",
    "            # list only .txt files (case-insensitive)\n",
    "            txt_files = [n for n in z.namelist() if n.lower().endswith(\".txt\")]\n",
    "            if not txt_files:\n",
    "                continue\n",
    "\n",
    "            random.shuffle(txt_files)\n",
    "            chosen = txt_files[:k_per_zip]\n",
    "\n",
    "            for fname in chosen:\n",
    "                try:\n",
    "                    raw = z.read(fname)\n",
    "                except KeyError:\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # Attempt UTF-8, fall back to latin-1\n",
    "                    text = raw.decode(\"utf-8\")\n",
    "                except UnicodeDecodeError:\n",
    "                    text = raw.decode(\"latin-1\")\n",
    "                    \n",
    "                text = text.strip()\n",
    "                if not text:\n",
    "                    continue\n",
    "                \n",
    "                # Use the filename as the title, removing the extension\n",
    "                title = Path(fname).stem\n",
    "                \n",
    "                # --- SIMPLIFIED SAMPLE DICT ---\n",
    "                sample = {\n",
    "                    \"text\": text,\n",
    "                    \"title\": title,\n",
    "                }\n",
    "                # ------------------------------\n",
    "                yield sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- USER CONFIG ----------\n",
    "DATA_DIR = \"../data/burialgoods\"       # folder containing many .zip files\n",
    "N_ZIPS = 10                            # pick N zip files\n",
    "K_PER_ZIP = 50                       # up to K .txt files per zip\n",
    "REPO_ID = \"eZWALT/burialgoods-pretraining-corpus\" \n",
    "PRIVATE_REPO = False\n",
    "LANGUAGE = \"en\"\n",
    "SEED = 42\n",
    "# ---------------------------------\n",
    "random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 9 zip files (out of 9)\n",
      "Building and pushing HF Dataset object...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Zips: 100%|██████████| 9/9 [00:00<00:00, 621.11it/s]\n",
      "Generating train split: 147 examples [00:00, 7072.62 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 147 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Select zip files\n",
    "zip_files = sorted(Path(DATA_DIR).glob(\"*.zip\"))\n",
    "if not zip_files:\n",
    "    raise FileNotFoundError(f\"No .zip files found in {DATA_DIR}\")\n",
    "\n",
    "if len(zip_files) <= N_ZIPS:\n",
    "    selected = zip_files\n",
    "else:\n",
    "    selected = random.sample(list(zip_files), N_ZIPS)\n",
    "    \n",
    "print(f\"Selected {len(selected)} zip files (out of {len(zip_files)})\")\n",
    "\n",
    "# 2. Define features and setup generator arguments\n",
    "# --- SIMPLIFIED FEATURES ---\n",
    "features = Features({\n",
    "    \"text\": Value(\"string\"),\n",
    "    \"title\": Value(\"string\"),\n",
    "})\n",
    "# ---------------------------\n",
    "\n",
    "generator_func = iter_samples_from_zips\n",
    "\n",
    "# Define the keyword arguments needed for the function\n",
    "generator_kwargs = {\n",
    "    \"data_dir\": Path(DATA_DIR),\n",
    "    \"selected_zips\": selected,\n",
    "    \"k_per_zip\": K_PER_ZIP\n",
    "}\n",
    "\n",
    "# 3. Create Hugging Face Dataset from the callable\n",
    "print(\"Building and pushing HF Dataset object...\")\n",
    "ds = Dataset.from_generator(\n",
    "    generator=generator_func, \n",
    "    gen_kwargs=generator_kwargs, \n",
    "    features=features\n",
    ")\n",
    "print(f\"Dataset size: {ds.num_rows} samples.\")\n",
    "\n",
    "# 4. Push to Hugging Face Hub\n",
    "print(f\"Pushing dataset to the Hub as {REPO_ID} ...\")\n",
    "ds.push_to_hub(REPO_ID, private=PRIVATE_REPO)\n",
    "print(\"Push complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Preview Pretraining Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "404 Client Error: Not Found for url: https://huggingface.co/collections/eZWALT/cursed-toxic-pretraining",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 189\u001b[39m\n\u001b[32m    185\u001b[39m     plt.close()\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fn\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m dataset_ids = \u001b[43mextract_dataset_ids_from_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCOLLECTION_URL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset_ids)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m dataset ids in collection (first 20 shown):\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dataset_ids[:\u001b[32m20\u001b[39m]:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 81\u001b[39m, in \u001b[36mextract_dataset_ids_from_collection\u001b[39m\u001b[34m(url)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract_dataset_ids_from_collection\u001b[39m(url):\n\u001b[32m     80\u001b[39m     r = requests.get(url, timeout=\u001b[32m20\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     \u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m     soup = BeautifulSoup(r.text, \u001b[33m\"\u001b[39m\u001b[33mhtml.parser\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     83\u001b[39m     dataset_ids = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/llm/lib/python3.12/site-packages/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1021\u001b[39m     http_error_msg = (\n\u001b[32m   1022\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1023\u001b[39m     )\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 404 Client Error: Not Found for url: https://huggingface.co/collections/eZWALT/cursed-toxic-pretraining"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "load_collection_and_visualize.py\n",
    "\n",
    "- Scrapes a Hugging Face collection page for dataset links.\n",
    "- Loads a small streaming sample from each dataset.\n",
    "- Applies a per-dataset small preprocessing function.\n",
    "- Visualizes safe stats: length histogram and label distribution.\n",
    "- ALWAYS masks text so we don't print toxic content verbatim.\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from itertools import islice\n",
    "from collections import Counter, defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import os\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "COLLECTION_URL = \"https://huggingface.co/collections/eZWALT/cursed-toxic-pretraining-corpora\"\n",
    "SAMPLES_PER_DATASET = 500          # how many items to inspect per dataset (streaming)\n",
    "OUTPUT_DIR = \"hf_collection_vis\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "# -----------------------------\n",
    "\n",
    "# --- Utilities ---\n",
    "def mask_text_safe(txt, max_len=200):\n",
    "    \"\"\"Mask alphabetic characters with bullets so we never print raw toxic words.\"\"\"\n",
    "    if not isinstance(txt, str):\n",
    "        return \"\"\n",
    "    masked = re.sub(r\"[A-Za-zÀ-ÖØ-öø-ÿ]\", \"•\", txt)  # preserve punctuation & spacing\n",
    "    return masked[:max_len] + (\"…\" if len(masked) > max_len else \"\")\n",
    "\n",
    "def first_text_field(example, candidate_fields=None):\n",
    "    \"\"\"Return the first plausible text field in a dataset example.\"\"\"\n",
    "    if candidate_fields is None:\n",
    "        candidate_fields = [\"text\", \"body\", \"selftext\", \"comment\", \"post\", \"sentence\", \"content\", \"review\", \"caption\"]\n",
    "    for f in candidate_fields:\n",
    "        if f in example and isinstance(example[f], str) and example[f].strip():\n",
    "            return example[f]\n",
    "    # fallback: find the first str valued field\n",
    "    for k, v in example.items():\n",
    "        if isinstance(v, str) and v.strip():\n",
    "            return v\n",
    "    return \"\"\n",
    "\n",
    "# --- Per-dataset small preprocessing functions ---\n",
    "# Add or override functions in this dict keyed by dataset_id (short id like 'hate_speech_offensive' or 'user/dsname')\n",
    "def default_preprocess(example):\n",
    "    text = first_text_field(example)\n",
    "    tlen = len(text.split())\n",
    "    return {\"masked_preview\": mask_text_safe(text, max_len=240), \"length_tokens\": tlen}\n",
    "\n",
    "def reddit_preprocess(example):\n",
    "    # reddit style datasets commonly have 'selftext' or 'body'\n",
    "    text = example.get(\"selftext\") or example.get(\"body\") or first_text_field(example)\n",
    "    tlen = len(text.split()) if text else 0\n",
    "    return {\"masked_preview\": mask_text_safe(text), \"length_tokens\": tlen}\n",
    "\n",
    "def hate_speech_preprocess(example):\n",
    "    # often have 'text' and 'label'\n",
    "    text = example.get(\"text\") or first_text_field(example)\n",
    "    label = example.get(\"label\", example.get(\"labels\", None))\n",
    "    return {\"masked_preview\": mask_text_safe(text), \"length_tokens\": len(text.split()) if text else 0, \"label\": label}\n",
    "\n",
    "# Map dataset slug (partial match) → preprocessing function\n",
    "PREPROCESS_MAP = {\n",
    "    \"reddit\": reddit_preprocess,\n",
    "    \"hate_speech_offensive\": hate_speech_preprocess,\n",
    "    \"hate-offensive\": hate_speech_preprocess,\n",
    "    # add more mappings as you need, keys can be substrings matched against dataset id\n",
    "}\n",
    "\n",
    "# --- Step 1: scrape collection page for dataset links ---\n",
    "def extract_dataset_ids_from_collection(url):\n",
    "    r = requests.get(url, timeout=20)\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    dataset_ids = []\n",
    "    # links to datasets often include '/datasets/' or '/viewer' pages which include dataset slugs\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"]\n",
    "        if href.startswith(\"/datasets/\"):\n",
    "            # /datasets/username/dsname or /datasets/dsname\n",
    "            dataset_ids.append(href.split(\"/datasets/\")[1].strip(\"/\"))\n",
    "        elif \"/datasets/\" in href:\n",
    "            dataset_ids.append(href.split(\"/datasets/\")[-1].strip(\"/\"))\n",
    "        # some collection items show a 'Viewer' link which may point to '/datasets/USER/DS/viewer' etc.\n",
    "    # dedupe preserving order\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for d in dataset_ids:\n",
    "        if d and d not in seen:\n",
    "            seen.add(d)\n",
    "            out.append(d)\n",
    "    return out\n",
    "\n",
    "# --- Step 2: load streaming sample and preprocess ---\n",
    "def load_and_analyze_dataset(dataset_id, n_samples=SAMPLES_PER_DATASET):\n",
    "    print(f\"\\n=== Processing dataset: {dataset_id} ===\")\n",
    "    preprocess_fn = default_preprocess\n",
    "    for key, fn in PREPROCESS_MAP.items():\n",
    "        if key.lower() in dataset_id.lower():\n",
    "            preprocess_fn = fn\n",
    "            break\n",
    "\n",
    "    # attempt to stream 'train' split first, then fallback to default\n",
    "    splits_to_try = [\"train\", \"valid\", \"test\", None]\n",
    "    results = []\n",
    "    last_exception = None\n",
    "    for sp in splits_to_try:\n",
    "        try:\n",
    "            if sp:\n",
    "                ds = load_dataset(dataset_id, split=sp, streaming=True)\n",
    "            else:\n",
    "                ds = load_dataset(dataset_id, streaming=True)\n",
    "            # gather up to n_samples\n",
    "            for item in islice(ds, n_samples):\n",
    "                try:\n",
    "                    proc = preprocess_fn(item)\n",
    "                    results.append(proc)\n",
    "                except Exception:\n",
    "                    # fallback to default preprocess for this item\n",
    "                    try:\n",
    "                        results.append(default_preprocess(item))\n",
    "                    except Exception:\n",
    "                        continue\n",
    "            break\n",
    "        except Exception as e:\n",
    "            last_exception = e\n",
    "            continue\n",
    "    if not results:\n",
    "        print(f\"  ! Could not load any samples for {dataset_id} (error: {last_exception})\")\n",
    "        return None\n",
    "\n",
    "    # compute some safe stats\n",
    "    lengths = [r.get(\"length_tokens\", 0) for r in results]\n",
    "    avg_len = sum(lengths) / len(lengths) if lengths else 0\n",
    "    pct_short = sum(1 for L in lengths if L <= 5) / len(lengths)\n",
    "    label_counts = Counter([r.get(\"label\") for r in results if \"label\" in r and r.get(\"label\") is not None])\n",
    "\n",
    "    # prepare a tiny report (masked)\n",
    "    masked_examples = [r[\"masked_preview\"] for r in results[:5]]\n",
    "\n",
    "    report = {\n",
    "        \"dataset_id\": dataset_id,\n",
    "        \"n_samples_examined\": len(results),\n",
    "        \"avg_length_tokens\": avg_len,\n",
    "        \"pct_short_<=5_tokens\": pct_short,\n",
    "        \"label_counts\": dict(label_counts),\n",
    "        \"masked_examples\": masked_examples,\n",
    "        \"lengths\": lengths,\n",
    "    }\n",
    "    return report\n",
    "\n",
    "# --- Step 3: visualizations (safe) ---\n",
    "def plot_length_histogram(lengths, dataset_id):\n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.hist(lengths, bins=30)\n",
    "    plt.title(f\"Length distribution (tokens) — {dataset_id}\")\n",
    "    plt.xlabel(\"Tokens\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    fn = os.path.join(OUTPUT_DIR, f\"{dataset_id.replace('/', '_')}_len_hist.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fn)\n",
    "    plt.close()\n",
    "    return fn\n",
    "\n",
    "def plot_label_distribution(label_counts, dataset_id):\n",
    "    if not label_counts:\n",
    "        return None\n",
    "    labels = list(label_counts.keys())\n",
    "    values = [label_counts[k] for k in labels]\n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.bar(range(len(labels)), values)\n",
    "    plt.title(f\"Label distribution — {dataset_id}\")\n",
    "    plt.xticks(range(len(labels)), [str(x) for x in labels], rotation=45)\n",
    "    plt.tight_layout()\n",
    "    fn = os.path.join(OUTPUT_DIR, f\"{dataset_id.replace('/', '_')}_label_dist.png\")\n",
    "    plt.savefig(fn)\n",
    "    plt.close()\n",
    "    return fn\n",
    "\n",
    "\n",
    "dataset_ids = extract_dataset_ids_from_collection(COLLECTION_URL)\n",
    "print(f\"Found {len(dataset_ids)} dataset ids in collection (first 20 shown):\")\n",
    "for d in dataset_ids[:20]:\n",
    "    print(\"  -\", d)\n",
    "reports = []\n",
    "for dsid in tqdm(dataset_ids, desc=\"Datasets\"):\n",
    "    rep = load_and_analyze_dataset(dsid, n_samples=SAMPLES_PER_DATASET)\n",
    "    if rep is None:\n",
    "        continue\n",
    "    # write a small JSON report\n",
    "    import json\n",
    "    outpath = os.path.join(OUTPUT_DIR, f\"{dsid.replace('/', '_')}_report.json\")\n",
    "    with open(outpath, \"w\", encoding=\"utf-8\") as fh:\n",
    "        json.dump({k: v for k,v in rep.items() if k != \"lengths\"}, fh, indent=2)\n",
    "    # plots\n",
    "    plot_length_histogram(rep[\"lengths\"], dsid)\n",
    "    plot_label_distribution(rep[\"label_counts\"], dsid)\n",
    "    print(f\"  -> examined {rep['n_samples_examined']} samples; avg tokens {rep['avg_length_tokens']:.1f}\")\n",
    "    print(\"     masked previews (first 3):\")\n",
    "    for m in rep[\"masked_examples\"][:3]:\n",
    "        print(\"       \", m)\n",
    "    reports.append(rep)\n",
    "print(f\"\\nAll done. Reports and plots saved in {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Preview Cursed Toxic Pretraining Corpora"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
